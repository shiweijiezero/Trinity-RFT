@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv},
  year={2020}
}

@article{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv},
  year={2020}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv},
  year={2021}
}

@article{bang2023multitask,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv},
  year={2023}
}

@article{guo2023close,
  title={How close is chatgpt to human experts? comparison corpus, evaluation, and detection},
  author={Guo, Biyang and Zhang, Xin and Wang, Ziyuan and Jiang, Minqi and Nie, Jinran and Ding, Yuxuan and Yue, Jianwei and Wu, Yupeng},
  journal={arXiv},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={NeurIPS},
  volume={35},
  pages={27730--27744},
  year={2022}
}



@article{cao2020factual,
  title={Factual error correction for abstractive summarization models},
  author={Cao, Meng and Dong, Yue and Wu, Jiapeng and Cheung, Jackie Chi Kit},
  journal={arXiv},
  year={2020}
}

@article{raunak2021curious,
  title={The curious case of hallucinations in neural machine translation},
  author={Raunak, Vikas and Menezes, Arul and Junczys-Dowmunt, Marcin},
  journal={arXiv},
  year={2021}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv},
  year={2022}
}

@article{he2022rethinking,
  title={Rethinking with retrieval: Faithful large language model inference},
  author={He, Hangfeng and Zhang, Hongming and Roth, Dan},
  journal={arXiv},
  year={2022}
}

@article{shen2023chatgpt,
  title={In chatgpt we trust? measuring and characterizing the reliability of chatgpt},
  author={Shen, Xinyue and Chen, Zeyuan and Backes, Michael and Zhang, Yang},
  journal={arXiv},
  year={2023}
}

@article{li2023chatgpt,
  title={Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks},
  author={Li, Xianzhi and Zhu, Xiaodan and Ma, Zhiqiang and Liu, Xiaomo and Shah, Sameena},
  journal={arXiv},
  year={2023}
}

@inproceedings{cai2022recent,
  title={Recent advances in retrieval-augmented text generation},
  author={Cai, Deng and Wang, Yan and Liu, Lemao and Shi, Shuming},
  booktitle={ACM SIGIR},
  pages={3417--3419},
  year={2022}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={NeurIPS},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{zhu2021retrieving,
  title={Retrieving and reading: A comprehensive survey on open-domain question answering},
  author={Zhu, Fengbin and Lei, Wenqiang and Wang, Chao and Zheng, Jianming and Poria, Soujanya and Chua, Tat-Seng},
  journal={arXiv},
  year={2021}
}

@article{zhang2022survey,
  title={A survey for efficient open domain question answering},
  author={Zhang, Qin and Chen, Shangsi and Xu, Dongkuan and Cao, Qingqing and Chen, Xiaojun and Cohn, Trevor and Fang, Meng},
  journal={arXiv},
  year={2022}
}

@article{FiD,
  title={Leveraging passage retrieval with generative models for open domain question answering},
  author={Izacard, Gautier and Grave, Edouard},
  journal={arXiv},
  year={2020}
}

@inproceedings{FiE,
  title={FiE: Building a Global Probability Space by Leveraging Early Fusion in Encoder for Open-Domain Question Answering},
  author={Kedia, Akhil and Zaidi, Mohd Abbas and Lee, Haejun},
  booktitle={EMNLP},
  pages={4246--4260},
  year={2022}
}

@article{R2D2,
  title={R2-D2: A modular baseline for open-domain question answering},
  author={Fajcik, Martin and Docekal, Martin and Ondrej, Karel and Smrz, Pavel},
  journal={arXiv},
  year={2021}
}

@inproceedings{REALM,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={ICML},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}

@article{DPR,
  title={Dense passage retrieval for open-domain question answering},
  author={Karpukhin, Vladimir and O{\u{g}}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  journal={arXiv},
  year={2020}
}

@article{REPLUG,
  title={Replug: Retrieval-augmented black-box language models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv},
  year={2023}
}

@article{atlas,
  title={Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  journal={arXiv},
  year={2022}
}

@article{RALM,
  title={In-context retrieval-augmented language models},
  author={Ram, Ori and Levine, Yoav and Dalmedigos, Itay and Muhlgay, Dor and Shashua, Amnon and Leyton-Brown, Kevin and Shoham, Yoav},
  journal={arXiv},
  year={2023}
}

@inproceedings{RETRO,
  title={Improving language models by retrieving from trillions of tokens},
  author={Borgeaud, Sebastian and Mensch, Arthur and Hoffmann, Jordan and Cai, Trevor and Rutherford, Eliza and Millican, Katie and Van Den Driessche, George Bm and Lespiau, Jean-Baptiste and Damoc, Bogdan and Clark, Aidan and others},
  booktitle={ICML},
  pages={2206--2240},
  year={2022},
  organization={PMLR}
}

@article{luo2023empirical,
  title={An Empirical Study of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={arXiv},
  pages={arXiv--2308},
  year={2023}
}

@article{cai2018skeleton,
  title={Skeleton-to-response: Dialogue generation guided by retrieval memory},
  author={Cai, Deng and Wang, Yan and Bi, Victoria and Tu, Zhaopeng and Liu, Xiaojiang and Lam, Wai and Shi, Shuming},
  journal={arXiv},
  year={2018}
}

@article{maynez2020faithfulness,
  title={On faithfulness and factuality in abstractive summarization},
  author={Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  journal={arXiv},
  year={2020}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv},
  year={2023}
}

@article{bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv},
  year={2022}
}

@article{baichuan,
  title={Baichuan 2: Open large-scale language models},
  author={Yang, Aiyuan and Xiao, Bin and Wang, Bingning and Zhang, Borong and Yin, Chao and Lv, Chenxu and Pan, Da and Wang, Dian and Yan, Dong and Yang, Fan and others},
  journal={arXiv},
  year={2023}
}

@article{contriever,
  title={Unsupervised dense information retrieval with contrastive learning},
  author={Izacard, Gautier and Caron, Mathilde and Hosseini, Lucas and Riedel, Sebastian and Bojanowski, Piotr and Joulin, Armand and Grave, Edouard},
  journal={arXiv},
  year={2021}
}

@inproceedings{arivazhagan2023hybrid,
  title={Hybrid Hierarchical Retrieval for Open-Domain Question Answering},
  author={Arivazhagan, Manoj Ghuhan and Liu, Lan and Qi, Peng and Chen, Xinchi and Wang, William Yang and Huang, Zhiheng},
  booktitle={Findings of ACL},
  pages={10680--10689},
  year={2023}
}

@article{bm25,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@article{rome,
  title={Locating and Editing Factual Associations in {GPT}},
  author={Kevin Meng and David Bau and Alex Andonian and Yonatan Belinkov},
  journal={NeurIPS},
  volume={35},
  year={2022}
}

@article{ORQA,
  title={Latent retrieval for weakly supervised open domain question answering},
  author={Lee, Kenton and Chang, Ming-Wei and Toutanova, Kristina},
  journal={arXiv},
  year={2019}
}

@article{NQ,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={TACL},
  volume={7},
  pages={453--466},
  year={2019},
}

@article{TQA,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv},
  year={2017}
}

@article{PALM,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv},
  year={2022}
}

@article{Codex,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv},
  year={2021}
}

@article{chinchilla,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv},
  year={2022}
}

@article{squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv},
  year={2016}
}

@book{es,
  title={Elasticsearch: the definitive guide: a distributed real-time search and analytics engine},
  author={Gormley, Clinton and Tong, Zachary},
  year={2015},
  publisher={" O'Reilly Media, Inc."}
}

@article{FAISS,
  title={Billion-scale similarity search with gpus},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  volume={7},
  number={3},
  pages={535--547},
  year={2019},
  publisher={IEEE}
}

@article{UnitedQA,
  title={UnitedQA: A hybrid approach for open domain question answering},
  author={Cheng, Hao and Shen, Yelong and Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  journal={arXiv},
  year={2021}
}

@article{dhr,
  title={Dense hierarchical retrieval for open-domain question answering},
  author={Liu, Ye and Hashimoto, Kazuma and Zhou, Yingbo and Yavuz, Semih and Xiong, Caiming and Yu, Philip S},
  journal={arXiv},
  year={2021}
}

@article{bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv},
  year={2018}
}

@article{roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv},
  year={2019}
}

@article{albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv},
  year={2019}
}

@article{kobayashi2000information,
  title={Information retrieval on the web},
  author={Kobayashi, Mei and Takeda, Koichi},
  journal={ACM computing surveys (CSUR)},
  volume={32},
  number={2},
  pages={144--173},
  year={2000},
  publisher={ACM New York, NY, USA}
}

@article{baradaran2022survey,
  title={A survey on machine reading comprehension systems},
  author={Baradaran, Razieh and Ghiasi, Razieh and Amirkhani, Hossein},
  journal={Natural Language Engineering},
  volume={28},
  number={6},
  pages={683--732},
  year={2022},
  publisher={Cambridge University Press}
}

@article{kg-fid,
  title={Kg-fid: Infusing knowledge graph in fusion-in-decoder for open-domain question answering},
  author={Yu, Donghan and Zhu, Chenguang and Fang, Yuwei and Yu, Wenhao and Wang, Shuohang and Xu, Yichong and Ren, Xiang and Yang, Yiming and Zeng, Michael},
  journal={arXiv},
  year={2021}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv},
  year={2023}
}

@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{gpt1,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Neurips},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{t5,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{MTEB,
  doi = {10.48550/ARXIV.2210.07316},
  url = {https://arxiv.org/abs/2210.07316},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  title = {MTEB: Massive Text Embedding Benchmark},
  publisher = {arXiv},
  journal={arXiv},  
  year = {2022}
}

@misc{SFR-embedding-2,
  title = {SFR-Embedding-2: Advanced Text Embedding with Multi-stage Training},
  author = {Rui Meng and Ye Liu and Shafiq Rayhan Joty and Caiming Xiong and Yingbo Zhou and Semih Yavuz},
  year = {2024},
  url = {https://huggingface.co/Salesforce/SFR-Embedding-2_R}
}

@article{liu2024lost,
  title={Lost in the middle: How language models use long contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={TACL},
  volume={12},
  pages={157--173},
  year={2024},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{shi2023large,
  title={Large language models can be easily distracted by irrelevant context},
  author={Shi, Freda and Chen, Xinyun and Misra, Kanishka and Scales, Nathan and Dohan, David and Chi, Ed H and Sch{\"a}rli, Nathanael and Zhou, Denny},
  booktitle={ICML},
  pages={31210--31227},
  year={2023},
  organization={PMLR}
}

@inproceedings{yoran2024making,
  title={Making Retrieval-Augmented Language Models Robust to Irrelevant Context},
  author={Yoran, Ori and Wolfson, Tomer and Ram, Ori and Berant, Jonathan},
  booktitle={ICLR},
  year={2024},
}

@article{wu2024instructing,
  title={Instructing large language models to identify and ignore irrelevant conditions},
  author={Wu, Zhenyu and Shen, Chao and Jiang, Meng},
  journal={arXiv},
  year={2024}
}

@article{liu2024ra,
  title={RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback},
  author={Liu, Yanming and Peng, Xinyue and Zhang, Xuhong and Liu, Weihao and Yin, Jianwei and Cao, Jiannan and Du, Tianyu},
  journal={arXiv},
  year={2024}
}

@article{chuang2023dola,
  title={Dola: Decoding by contrasting layers improves factuality in large language models},
  author={Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng},
  journal={arXiv},
  year={2023}
}

@article{survey1,
  title={Retrieval-augmented generation for ai-generated content: A survey},
  author={Zhao, Penghao and Zhang, Hailin and Yu, Qinhan and Wang, Zhengren and Geng, Yunteng and Fu, Fangcheng and Yang, Ling and Zhang, Wentao and Cui, Bin},
  journal={arXiv},
  year={2024}
}

@article{survey2,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen},
  journal={arXiv},
  year={2023}
}

@article{survey3,
  title={Retrieval-Augmented Generation for Natural Language Processing: A Survey},
  author={Wu, Shangyu and Xiong, Ying and Cui, Yufei and Wu, Haolun and Chen, Can and Yuan, Ye and Huang, Lianming and Liu, Xue and Kuo, Tei-Wei and Guan, Nan and others},
  journal={arXiv},
  year={2024}
}

@article{liu2023recap,
  title={Recap: Retrieval-enhanced context-aware prefix encoder for personalized dialogue response generation},
  author={Liu, Shuai and Cho, Hyundong J and Freedman, Marjorie and Ma, Xuezhe and May, Jonathan},
  journal={arXiv},
  year={2023}
}

@article{khandelwal2019generalization,
  title={Generalization through memorization: Nearest neighbor language models},
  author={Khandelwal, Urvashi and Levy, Omer and Jurafsky, Dan and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv},
  year={2019}
}

@article{huang2023k,
  title={$ k $ NN-Adapter: Efficient Domain Adaptation for Black-Box Language Models},
  author={Huang, Yangsibo and Liu, Daogao and Zhong, Zexuan and Shi, Weijia and Lee, Yin Tat},
  journal={arXiv},
  year={2023}
}

@article{wang2023shall,
  title={Shall we pretrain autoregressive language models with retrieval? a comprehensive study},
  author={Wang, Boxin and Ping, Wei and Xu, Peng and McAfee, Lawrence and Liu, Zihan and Shoeybi, Mohammad and Dong, Yi and Kuchaiev, Oleksii and Li, Bo and Xiao, Chaowei and others},
  journal={arXiv},
  year={2023}
}

@article{wu2024improving,
  title={Improving natural language understanding with computation-efficient retrieval representation fusion},
  author={Wu, Shangyu and Xiong, Ying and Cui, Yufei and Liu, Xue and Tang, Buzhou and Kuo, Tei-Wei and Xue, Chun Jason},
  journal={arXiv},
  year={2024}
}

@article{bge,
  title={Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation},
  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
  journal={arXiv},
  year={2024}
}

@article{wang2023improving,
  title={Improving text embeddings with large language models},
  author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu},
  journal={arXiv},
  year={2023}
}

@article{behnamghader2024llm2vec,
  title={Llm2vec: Large language models are secretly powerful text encoders},
  author={BehnamGhader, Parishad and Adlakha, Vaibhav and Mosbach, Marius and Bahdanau, Dzmitry and Chapados, Nicolas and Reddy, Siva},
  journal={arXiv},
  year={2024}
}

@article{springer2024repetition,
  title={Repetition improves language model embeddings},
  author={Springer, Jacob Mitchell and Kotha, Suhas and Fried, Daniel and Neubig, Graham and Raghunathan, Aditi},
  journal={arXiv},
  year={2024}
}

@article{zhang2024comprehensive,
  title={A comprehensive study of knowledge editing for large language models},
  author={Zhang, Ningyu and Yao, Yunzhi and Tian, Bozhong and Wang, Peng and Deng, Shumin and Wang, Mengru and Xi, Zekun and Mao, Shengyu and Zhang, Jintian and Ni, Yuansheng and others},
  journal={arXiv},
  year={2024}
}

@article{nawrot2024dynamic,
  title={Dynamic memory compression: Retrofitting llms for accelerated inference},
  author={Nawrot, Piotr and {\L}a{\'n}cucki, Adrian and Chochowski, Marcin and Tarjan, David and Ponti, Edoardo M},
  journal={arXiv},
  year={2024}
}

@inproceedings{zhai2023stabilizing,
  title={Stabilizing transformer training by preventing attention entropy collapse},
  author={Zhai, Shuangfei and Likhomanenko, Tatiana and Littwin, Etai and Busbridge, Dan and Ramapuram, Jason and Zhang, Yizhe and Gu, Jiatao and Susskind, Joshua M},
  booktitle={ICML},
  pages={40770--40803},
  year={2023},
  organization={PMLR}
}

@article{fu2024attentionpattern, 
    title = "How Do Language Models put Attention Weights over Long Context?", 
    author = "Fu, Yao", 
    journal = "Yao Fu’s Notion", 
    year = "2024", 
    month = "Mar", 
    url = "https://yaofu.notion.site/How-Do-Language-Models-put-Attention-Weights-over-Long-Context-10250219d5ce42e8b465087c383a034e?pvs=4" 
}

@inproceedings{zhang2021drop,
  title={Drop redundant, shrink irrelevant: Selective knowledge injection for language pretraining.},
  author={Zhang, Ningyu and Deng, Shumin and Cheng, Xu and Chen, Xi and Zhang, Yichi and Zhang, Wei and Chen, Huajun and Center, Hangzhou Innovation},
  booktitle={IJCAI},
  pages={4007--4014},
  year={2021}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv},
  year={2023}
}

@article{strategyQA,
  title={Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies},
  author={Geva, Mor and Khashabi, Daniel and Segal, Elad and Khot, Tushar and Roth, Dan and Berant, Jonathan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={346--361},
  year={2021},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{HotpotQA,
  title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal={arXiv},
  year={2018}
}

@article{PopQA,
  title={When not to trust language models: Investigating effectiveness of parametric and non-parametric memories},
  author={Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv},
  year={2022}
}

@article{ho2020constructing,
  title={Constructing a multi-hop QA dataset for comprehensive evaluation of reasoning steps},
  author={Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and Aizawa, Akiko},
  journal={arXiv},
  year={2020}
}

@article{dubey2024llama,
  title={The Llama 3 Herd of Models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv},
  year={2024}
}

@article{asai2023self,
  title={Self-rag: Learning to retrieve, generate, and critique through self-reflection},
  author={Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2310.11511},
  year={2023}
}

@article{yu2024rankrag,
  title={RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs},
  author={Yu, Yue and Ping, Wei and Liu, Zihan and Wang, Boxin and You, Jiaxuan and Zhang, Chao and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv},
  year={2024}
}

@article{liu2024chatqa,
  title={Chatqa: Building gpt-4 level conversational qa models},
  author={Liu, Zihan and Ping, Wei and Roy, Rajarshi and Xu, Peng and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv},
  year={2024}
}

@article{ROPE,
  title={Roformer: Enhanced transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv},
  year={2023}
}

@inproceedings{hyeon2023scratching,
  title={Scratching visual transformer's back with uniform attention},
  author={Hyeon-Woo, Nam and Yu-Ji, Kim and Heo, Byeongho and Han, Dongyoon and Oh, Seong Joon and Oh, Tae-Hyun},
  booktitle={ICCV},
  pages={5807--5818},
  year={2023}
}

@article{mmlu,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv},
  year={2020}
}